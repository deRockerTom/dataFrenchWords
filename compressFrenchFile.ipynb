{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnaGtm7MwT4BC3R7NWnP7J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deRockerTom/dataFrenchWords/blob/main/compressFrenchFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/deRockerTom/dataFrenchWords\n",
        "!pip install textract\n",
        "!sudo apt-get install antiword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "scT0RXv3qF9z",
        "outputId": "f009779e-7e7d-4f95-cebd-d5e4699bf9b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dataFrenchWords'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 113 (delta 19), reused 0 (delta 0), pack-reused 78\u001b[K\n",
            "Receiving objects: 100% (113/113), 6.21 MiB | 27.75 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting python-pptx~=0.6.18\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 11.2 MB/s \n",
            "\u001b[?25hCollecting xlrd~=1.2.0\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from textract) (3.0.4)\n",
            "Collecting pdfminer.six==20191110\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 37.8 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4~=4.8.0\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 68.9 MB/s \n",
            "\u001b[?25hCollecting argcomplete~=1.10.0\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting SpeechRecognition~=3.8.1\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting extract-msg<=0.29.*\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting docx2txt~=0.8\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Collecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.3.2.post1)\n",
            "Collecting ebcdic>=1.1.1\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 63.6 MB/s \n",
            "\u001b[?25hCollecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting tzlocal>=2.1\n",
            "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
            "Collecting compressed-rtf>=1.0.6\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "Collecting imapclient==2.1.0\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 59.3 MB/s \n",
            "\u001b[?25hCollecting pytz-deprecation-shim\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 11.6 MB/s \n",
            "\u001b[?25hCollecting tzdata\n",
            "  Downloading tzdata-2022.1-py2.py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 72.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile, python-pptx\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=95f1018db25a2f4fdd90e840f651d55b57c82250cd225e3d9e6480a4d0b4effb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6204 sha256=2729ae94a7d4496745e32d7e2e4d28bdcd9654cfccb3c4dbec6828ad42f1ea50\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/33/88/88ceee84d1b74b391c086bc594d3fcf80800decfbd6e1ff565\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=8ff72e6b2cb46d21759a3b1c54e61a629793fcbd699724311307895b8642fc9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/53/e6/37d90ccb3ad1a3ca98d2b17107e9fda401a7c541ea1eb6a65a\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470951 sha256=2b8c0128a688f22713867cb2229615038b1072f6c574f1f9f0ddc5f1524f6fb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/ab/f4/52560d0d4bd4055e9261c6df6e51c7b56c2b23cca3dee811a3\n",
            "Successfully built docx2txt compressed-rtf olefile python-pptx\n",
            "Installing collected packages: tzdata, backports.zoneinfo, six, pytz-deprecation-shim, XlsxWriter, tzlocal, pycryptodome, olefile, imapclient, ebcdic, compressed-rtf, xlrd, SpeechRecognition, python-pptx, pdfminer.six, extract-msg, docx2txt, beautifulsoup4, argcomplete, textract\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 1.5.1\n",
            "    Uninstalling tzlocal-1.5.1:\n",
            "      Successfully uninstalled tzlocal-1.5.1\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.31.6 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.3 argcomplete-1.10.3 backports.zoneinfo-0.2.1 beautifulsoup4-4.8.2 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.15.0 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 six-1.12.0 textract-1.6.5 tzdata-2022.1 tzlocal-4.2 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  antiword\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 128 kB of archives.\n",
            "After this operation, 633 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 antiword amd64 0.37-11build1 [128 kB]\n",
            "Fetched 128 kB in 0s (309 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package antiword.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../antiword_0.37-11build1_amd64.deb ...\n",
            "Unpacking antiword (0.37-11build1) ...\n",
            "Setting up antiword (0.37-11build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text = textract.process(\"dataFrenchWords/maupassant_bel_ami_illustre.doc\")\n",
        "text = text.decode(\"utf-8\")\n",
        "text = text.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "m-h39iUgJbvv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dict with file\n",
        "import os\n",
        "\n",
        "def getDictWithFile(file):\n",
        "  with open(file, 'r') as f:\n",
        "    dic = {}\n",
        "    key = \"\"\n",
        "    value = \"\"\n",
        "    isKey = True\n",
        "    for l in f.readlines():\n",
        "      for c in l:\n",
        "        if c == 'ក':\n",
        "          isKey = False\n",
        "        elif c == 'ខ':\n",
        "          isKey = True\n",
        "          dic[key] = int(value)\n",
        "          key = \"\"\n",
        "          value = \"\"\n",
        "        else:\n",
        "          if isKey:\n",
        "            key = key + c\n",
        "          else:\n",
        "            value = value + c\n",
        "    value = value + \"\\n\"\n",
        "  return dic\n",
        "\n",
        "dicByHugo = getDictWithFile('dataFrenchWords/dicByHugo')\n",
        "dicByHugoMax = getDictWithFile('dataFrenchWords/dicByHugoMax')\n",
        "dictByHugoMax10 = getDictWithFile('dataFrenchWords/dicByHugoMax10')\n",
        "dictByHugoMax50 = getDictWithFile('dataFrenchWords/dicByHugoMax50')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D-79WIkptQUQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToBinary(number):\n",
        "  binary = ''\n",
        "  while number > 0:\n",
        "    binary = str(number % 2) + binary\n",
        "    number = number // 2\n",
        "  return binary\n",
        "def convertToNineBits(bits):\n",
        "  n = len(bits)\n",
        "  if n < 9:\n",
        "    k = 9 - n\n",
        "    return str('0') * k + bits\n",
        "  elif n == 9:\n",
        "    return bits\n",
        "  else:\n",
        "    print(\"Attention : un des encodages est trop grand (>9)\")\n",
        "    print(bits)\n",
        "\n",
        "convertToNineBits(convertToBinary(255))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3osK_tKk8uSu",
        "outputId": "80c3170b-9dea-4cda-cd6b-0835a74be134"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'011111111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# open csv\n",
        "dic = {}\n",
        "with open('dataFrenchWords/french-word-list-verbs.csv', 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  #drop first 3 lines\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  #create dictionary\n",
        "  for row in reader:\n",
        "    if row[1] in dic:\n",
        "      dic[row[1]] += int(row[2])\n",
        "    else:\n",
        "      dic[row[1]] = int(row[2])\n",
        "  csvfile.close()\n",
        "\n",
        "with open('dataFrenchWords/french-word-list-adjectives.csv', 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  #drop first 3 lines\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  #create dictionary\n",
        "  for row in reader:\n",
        "    if row[1] in dic:\n",
        "      dic[row[1]] += int(row[2])\n",
        "    else:\n",
        "      dic[row[1]] = int(row[2])\n",
        "  csvfile.close()\n",
        "\n",
        "with open('dataFrenchWords/french-word-list-nouns.csv', 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  #drop first 3 lines\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  #create dictionary\n",
        "  for row in reader:\n",
        "    if row[1] in dic:\n",
        "      dic[row[1]] += int(row[2])\n",
        "    else:\n",
        "      dic[row[1]] = int(row[2])\n",
        "  csvfile.close()\n",
        "\n",
        "# Ponctuation :\n",
        "\n",
        "dic[', '] = 150000\n",
        "dic['. '] = 150000\n",
        "\n",
        "dic = dict(sorted(dic.items(), key=lambda item: item[1], reverse=True))\n",
        "dic = {k: dic[k] for k in list(dic)[:256]} # In order to code the dictionnary on only 2 other bytes with most frequent words\n",
        "offset = 256\n",
        "i = 0\n",
        "for key in dic:\n",
        "  dic[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "dic2 = {}\n",
        "offset = 256\n",
        "for key in dicByHugo:\n",
        "  dic2[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "dic3 = {}\n",
        "offset = 256\n",
        "for key in dicByHugoMax:\n",
        "  dic3[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "dic4 = {}\n",
        "offset = 256\n",
        "for key in dictByHugoMax10:\n",
        "  dic4[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "dic5 = {}\n",
        "offset = 256\n",
        "for key in dictByHugoMax50:\n",
        "  dic5[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "\n",
        "print(dic5)\n",
        "\n",
        "reverted_dic = {v: k for k, v in dic.items()}\n",
        "reverted_dic2 = {v: k for k, v in dic2.items()}\n",
        "reverted_dic3 = {v: k for k, v in dic3.items()}\n",
        "reverted_dic4 = {v: k for k, v in dic4.items()}\n",
        "reverted_dic5 = {v: k for k, v in dic5.items()}\n",
        "\n",
        "print(reverted_dic5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awJCxj_ArQG9",
        "outputId": "21d4ff00-3b5d-4937-9c49-3a392f7cddc2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'  ': '100000000', 'e ': '100000001', 's ': '100000010', 't ': '100000011', ' d': '100000100', ', ': '100000101', 'ai': '100000110', 'es': '100000111', ' l': '100001000', 'le': '100001001', 'en': '100001010', 're': '100001011', 'ou': '100001100', 'de': '100001101', ' s': '100001110', 'nt': '100001111', 'it': '100010000', ' p': '100010001', 'on': '100010010', ' c': '100010011', 'an': '100010100', 'n ': '100010101', ' e': '100010110', 'ur': '100010111', 'te': '100011000', 'er': '100011001', 'is': '100011010', 'r ': '100011011', '\\n ': '100011100', ' a': '100011101', '\\n\\n': '100011110', 'a ': '100011111', 'qu': '100100000', '   ': '100100001', 'se': '100100010', 'ne': '100100011', 'et': '100100100', ' m': '100100101', 'me': '100100110', 'la': '100100111', ' de': '100101000', 'ie': '100101001', 'il': '100101010', 'e  ': '100101011', 'eu': '100101100', ' t': '100101101', 'ra': '100101110', 'ar': '100101111', 'in': '100110000', 'll': '100110001', 'ui': '100110010', 'l ': '100110011', 'es ': '100110100', 'un': '100110101', '. ': '100110110', '\\n  ': '100110111', ' f': '100111000', 'us': '100111001', '\\n\\n ': '100111010', '\\n\\n\\n': '100111011', 'co': '100111100', ' q': '100111101', ' r': '100111110', ' v': '100111111', 'tr': '101000000', 'pa': '101000001', 'ns': '101000010', 'ue': '101000011', 'it ': '101000100', 'oi': '101000101', 'ait': '101000110', 'e,': '101000111', '    ': '101001000', 'ce': '101001001', 'sa': '101001010', 'de ': '101001011', 've': '101001100', 'le ': '101001101', 'i ': '101001110', 'ta': '101001111', 'ut': '101010000', 'ri': '101010001', 'au': '101010010', 'u ': '101010011', 'ch': '101010100', 'em': '101010101', 'nd': '101010110', 's  ': '101010111', 'ti': '101011000', 'ir': '101011001', ' le': '101011010', '\\n   ': '101011011', 'ent': '101011100', 'ma': '101011101', 'ro': '101011110', '\\n\\n  ': '101011111', 'so': '101100000', 'po': '101100001', '\\n\\n\\n ': '101100010', ' u': '101100011', 'nt ': '101100100', 'el': '101100101', 't  ': '101100110', ' n': '101100111', 'or': '101101000', 'ss': '101101001', ' i': '101101010', ' b': '101101011', 's,': '101101100', ' qu': '101101101', '  d': '101101110', '.\\n': '101101111', 'om': '101110000', \"'a\": '101110001', 'av': '101110010', 'as': '101110011', 'vo': '101110100', 'va': '101110101', '\\n    ': '101110110', 'à ': '101110111', 'st': '101111000', '\\n\\n   ': '101111001', '\\n\\n\\n  ': '101111010', 'lle': '101111011', 'e.': '101111100', 'pr': '101111101', 'to': '101111110', '«\\xa0': '101111111', ' à': '110000000', ' j': '110000001', 'ne ': '110000010', 'da': '110000011', ' «': '110000100', '\\xa0»': '110000101', 'e, ': '110000110', ' de ': '110000111', ',  ': '110001000', 'al': '110001001', 'lu': '110001010', 'si': '110001011', 'ant': '110001100', ' et': '110001101', 'et ': '110001110', 'mm': '110001111', 'pe': '110010000', '\\xa0:': '110010001', 'ait ': '110010010', 'e\\n': '110010011', ' la': '110010100', '  l': '110010101', 'mo': '110010110', '\\n\\n    ': '110010111', '\\n\\n\\n   ': '110011000', ' pa': '110011001', ' un': '110011010', 'la ': '110011011', 't,': '110011100', 'di': '110011101', \"l'\": '110011110', 'he': '110011111', 'Il': '110100000', 'our': '110100001', '»\\n': '110100010', 'rt': '110100011', \"d'\": '110100100', 'que': '110100101', ' se': '110100110', 're ': '110100111', \"'e\": '110101000', ' I': '110101001', ' co': '110101010', 'na': '110101011', 'ux': '110101100', '\\n\\n\\n    ': '110101101', '.\\xa0': '110101110', 's, ': '110101111', 'e d': '110110000', 'ré': '110110001', 'nc': '110110010', 'rs': '110110011', 'ais': '110110100', 'les': '110110101', 'is ': '110110110', 'mi': '110110111', 's.': '110111000', '  s': '110111001', 'il ': '110111010', 'ge': '110111011', 'on ': '110111100', 'li': '110111101', 'su': '110111110', 'ét': '110111111', 'ur ': '111000000', 'fa': '111000001', '  p': '111000010', 'at': '111000011', '.\\n\\n': '111000100', ' g': '111000101', ' «\\xa0': '111000110', ' E': '111000111', ' so': '111001000', ':\\n': '111001001', ' en': '111001010', 'er ': '111001011', 'vi': '111001100', 'ous': '111001101', 'ns ': '111001110', 'us ': '111001111', 'ien': '111010000', ' à ': '111010001', 'e s': '111010010', 'ec': '111010011', ' et ': '111010100', 'ha': '111010101', 'tt': '111010110', 'e\\xa0': '111010111', 'x ': '111011000', 's\\n': '111011001', '  c': '111011010', 'ée': '111011011', ' é': '111011100', 'é ': '111011101', 'une': '111011110', 'en ': '111011111', ' po': '111100000', 'un ': '111100001', \"u'\": '111100010', 'pl': '111100011', 'e l': '111100100', ' il': '111100101', '  e': '111100110', 'mme': '111100111', 'n  ': '111101000', ' o': '111101001', ' la ': '111101010', 'no': '111101011', 'te ': '111101100', 'uv': '111101101', 'ue ': '111101110', 'nn': '111101111', 'me ': '111110000', ' re': '111110001', 'je': '111110010', 'tre': '111110011', 'bl': '111110100', '\\xa0»\\n': '111110101', 'du': '111110110', '»\\n\\n': '111110111', 't d': '111111000', '  «': '111111001', 't, ': '111111010', 'lo': '111111011', 'eur': '111111100', 'ui ': '111111101', ' Il': '111111110', ' vo': '111111111'}\n",
            "{'100000000': '  ', '100000001': 'e ', '100000010': 's ', '100000011': 't ', '100000100': ' d', '100000101': ', ', '100000110': 'ai', '100000111': 'es', '100001000': ' l', '100001001': 'le', '100001010': 'en', '100001011': 're', '100001100': 'ou', '100001101': 'de', '100001110': ' s', '100001111': 'nt', '100010000': 'it', '100010001': ' p', '100010010': 'on', '100010011': ' c', '100010100': 'an', '100010101': 'n ', '100010110': ' e', '100010111': 'ur', '100011000': 'te', '100011001': 'er', '100011010': 'is', '100011011': 'r ', '100011100': '\\n ', '100011101': ' a', '100011110': '\\n\\n', '100011111': 'a ', '100100000': 'qu', '100100001': '   ', '100100010': 'se', '100100011': 'ne', '100100100': 'et', '100100101': ' m', '100100110': 'me', '100100111': 'la', '100101000': ' de', '100101001': 'ie', '100101010': 'il', '100101011': 'e  ', '100101100': 'eu', '100101101': ' t', '100101110': 'ra', '100101111': 'ar', '100110000': 'in', '100110001': 'll', '100110010': 'ui', '100110011': 'l ', '100110100': 'es ', '100110101': 'un', '100110110': '. ', '100110111': '\\n  ', '100111000': ' f', '100111001': 'us', '100111010': '\\n\\n ', '100111011': '\\n\\n\\n', '100111100': 'co', '100111101': ' q', '100111110': ' r', '100111111': ' v', '101000000': 'tr', '101000001': 'pa', '101000010': 'ns', '101000011': 'ue', '101000100': 'it ', '101000101': 'oi', '101000110': 'ait', '101000111': 'e,', '101001000': '    ', '101001001': 'ce', '101001010': 'sa', '101001011': 'de ', '101001100': 've', '101001101': 'le ', '101001110': 'i ', '101001111': 'ta', '101010000': 'ut', '101010001': 'ri', '101010010': 'au', '101010011': 'u ', '101010100': 'ch', '101010101': 'em', '101010110': 'nd', '101010111': 's  ', '101011000': 'ti', '101011001': 'ir', '101011010': ' le', '101011011': '\\n   ', '101011100': 'ent', '101011101': 'ma', '101011110': 'ro', '101011111': '\\n\\n  ', '101100000': 'so', '101100001': 'po', '101100010': '\\n\\n\\n ', '101100011': ' u', '101100100': 'nt ', '101100101': 'el', '101100110': 't  ', '101100111': ' n', '101101000': 'or', '101101001': 'ss', '101101010': ' i', '101101011': ' b', '101101100': 's,', '101101101': ' qu', '101101110': '  d', '101101111': '.\\n', '101110000': 'om', '101110001': \"'a\", '101110010': 'av', '101110011': 'as', '101110100': 'vo', '101110101': 'va', '101110110': '\\n    ', '101110111': 'à ', '101111000': 'st', '101111001': '\\n\\n   ', '101111010': '\\n\\n\\n  ', '101111011': 'lle', '101111100': 'e.', '101111101': 'pr', '101111110': 'to', '101111111': '«\\xa0', '110000000': ' à', '110000001': ' j', '110000010': 'ne ', '110000011': 'da', '110000100': ' «', '110000101': '\\xa0»', '110000110': 'e, ', '110000111': ' de ', '110001000': ',  ', '110001001': 'al', '110001010': 'lu', '110001011': 'si', '110001100': 'ant', '110001101': ' et', '110001110': 'et ', '110001111': 'mm', '110010000': 'pe', '110010001': '\\xa0:', '110010010': 'ait ', '110010011': 'e\\n', '110010100': ' la', '110010101': '  l', '110010110': 'mo', '110010111': '\\n\\n    ', '110011000': '\\n\\n\\n   ', '110011001': ' pa', '110011010': ' un', '110011011': 'la ', '110011100': 't,', '110011101': 'di', '110011110': \"l'\", '110011111': 'he', '110100000': 'Il', '110100001': 'our', '110100010': '»\\n', '110100011': 'rt', '110100100': \"d'\", '110100101': 'que', '110100110': ' se', '110100111': 're ', '110101000': \"'e\", '110101001': ' I', '110101010': ' co', '110101011': 'na', '110101100': 'ux', '110101101': '\\n\\n\\n    ', '110101110': '.\\xa0', '110101111': 's, ', '110110000': 'e d', '110110001': 'ré', '110110010': 'nc', '110110011': 'rs', '110110100': 'ais', '110110101': 'les', '110110110': 'is ', '110110111': 'mi', '110111000': 's.', '110111001': '  s', '110111010': 'il ', '110111011': 'ge', '110111100': 'on ', '110111101': 'li', '110111110': 'su', '110111111': 'ét', '111000000': 'ur ', '111000001': 'fa', '111000010': '  p', '111000011': 'at', '111000100': '.\\n\\n', '111000101': ' g', '111000110': ' «\\xa0', '111000111': ' E', '111001000': ' so', '111001001': ':\\n', '111001010': ' en', '111001011': 'er ', '111001100': 'vi', '111001101': 'ous', '111001110': 'ns ', '111001111': 'us ', '111010000': 'ien', '111010001': ' à ', '111010010': 'e s', '111010011': 'ec', '111010100': ' et ', '111010101': 'ha', '111010110': 'tt', '111010111': 'e\\xa0', '111011000': 'x ', '111011001': 's\\n', '111011010': '  c', '111011011': 'ée', '111011100': ' é', '111011101': 'é ', '111011110': 'une', '111011111': 'en ', '111100000': ' po', '111100001': 'un ', '111100010': \"u'\", '111100011': 'pl', '111100100': 'e l', '111100101': ' il', '111100110': '  e', '111100111': 'mme', '111101000': 'n  ', '111101001': ' o', '111101010': ' la ', '111101011': 'no', '111101100': 'te ', '111101101': 'uv', '111101110': 'ue ', '111101111': 'nn', '111110000': 'me ', '111110001': ' re', '111110010': 'je', '111110011': 'tre', '111110100': 'bl', '111110101': '\\xa0»\\n', '111110110': 'du', '111110111': '»\\n\\n', '111111000': 't d', '111111001': '  «', '111111010': 't, ', '111111011': 'lo', '111111100': 'eur', '111111101': 'ui ', '111111110': ' Il', '111111111': ' vo'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class to build a tree defining all the possible  string in dic\n",
        "class SuffixTree():\n",
        "  class Node():\n",
        "    def __init__(self,char):\n",
        "      self.char = char\n",
        "      self.out = {}\n",
        "      self.isFinal = False\n",
        "    def __str__(self, level=0):\n",
        "      ret = \"  \" * level + str(self.char) + \"\\n\"\n",
        "      for c in self.out:\n",
        "        ret += self.out[c].__str__(level + 1)\n",
        "      return ret\n",
        "\n",
        "  def __init__(self, dic):\n",
        "    self.root = self.Node(None)\n",
        "    self.root.isFinal = True\n",
        "    for key in dic:\n",
        "      cursor = self.root\n",
        "      for c in key:\n",
        "        if c in cursor.out:\n",
        "          cursor = cursor.out[c]\n",
        "        else:\n",
        "          node = self.Node(c)\n",
        "          cursor.out[c] = node\n",
        "          cursor = node\n",
        "      cursor.isFinal = True\n",
        "  def longestPrefix(self, s):\n",
        "    cursor = self.root\n",
        "    word = ''\n",
        "    temp = ''\n",
        "    end = False\n",
        "    i = 0\n",
        "    length = len(s)\n",
        "    while not(end) and i < length:\n",
        "      if s[i] in cursor.out:\n",
        "        temp = temp + s[i]\n",
        "        cursor = cursor.out[s[i]]\n",
        "        if cursor.isFinal:\n",
        "          word = temp\n",
        "        i += 1\n",
        "      else:\n",
        "        end = True\n",
        "    return word\n",
        "  def __str__(self):\n",
        "    return f'{self.root}'\n",
        "\n",
        "s = SuffixTree({'ab' : 1, 'abc': 2, 'coucou' : 5, 'aba' : 4, 'aaa' : 8})\n",
        "print(s)\n",
        "s.longestPrefix('abdc')\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "ND_fSFhFq51O",
        "outputId": "29c50141-6e80-49f5-ec04-a2edb5534198"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "  a\n",
            "    b\n",
            "      c\n",
            "      a\n",
            "    a\n",
            "      a\n",
            "  c\n",
            "    o\n",
            "      u\n",
            "        c\n",
            "          o\n",
            "            u\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ab'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree = SuffixTree(dic)\n",
        "tree2 = SuffixTree(dic2)\n",
        "tree3 = SuffixTree(dic3)\n",
        "tree4 = SuffixTree(dic4)\n",
        "tree5 = SuffixTree(dic5)"
      ],
      "metadata": {
        "id": "FgOeftRxAILs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compress(string, tree, dic):\n",
        "  i = 0\n",
        "  ret = ''\n",
        "  while i<len(string):\n",
        "    temp = tree.longestPrefix(string[i:])\n",
        "    n = len(temp)\n",
        "    if n == 0:\n",
        "      bit = convertToNineBits(convertToBinary(ord(string[i])))\n",
        "      ret += bit\n",
        "      i += 1\n",
        "    else:\n",
        "      bit = dic[temp]\n",
        "      ret += bit\n",
        "      i += n\n",
        "  return ret\n",
        "\n",
        "\n",
        "\n",
        "def decompress(string, dic):\n",
        "  ret = ''\n",
        "  for i in range(0, len(string) - 1, 9):\n",
        "    if int(string[i : i + 9], 2) > 255:\n",
        "      ret += dic[string[i : i + 9]]\n",
        "    else:\n",
        "      ret += chr(int(string[i : i + 9], 2))\n",
        "  return ret"
      ],
      "metadata": {
        "id": "d22Anv76pQBY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi process version\n",
        "\n",
        "import multiprocessing\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "\n",
        "def compressChunk(string, tree, dic):\n",
        "  i = 0\n",
        "  ret = ''\n",
        "  while i<len(string):\n",
        "    temp = tree.longestPrefix(string[i:])\n",
        "    n = len(temp)\n",
        "    if n == 0:\n",
        "      bit = convertToNineBits(convertToBinary(ord(string[i])))\n",
        "      ret += bit\n",
        "      i += 1\n",
        "    else:\n",
        "      bit = dic[temp]\n",
        "      ret += bit\n",
        "      i += n\n",
        "  return ret\n",
        "\n",
        "def compressMulti(string, tree, dic):\n",
        "  chunks, chunk_size = len(string), len(string)//os.cpu_count()\n",
        "  string_list = [ string[i:i+chunk_size] for i in range(0, chunks, chunk_size) ]\n",
        "  with Pool() as p:\n",
        "    ret_list = p.map(partial(compressChunk, tree=tree, dic=dic), string_list)\n",
        "  ret = ''.join(ret_list)\n",
        "  return ret\n",
        "\n",
        "def decompressChunk(string, dic):\n",
        "  ret = ''\n",
        "  for i in range(0, len(string) - 1, 9):\n",
        "    if int(string[i : i + 9], 2) > 255:\n",
        "      ret += dic[string[i : i + 9]]\n",
        "    else:\n",
        "      ret += chr(int(string[i : i + 9], 2))\n",
        "  return ret\n",
        "\n",
        "def decompressMulti(string, dic):\n",
        "  chunks, chunk_size = len(string), (len(string)//(os.cpu_count() * 9)) * 9\n",
        "  string_list = [ string[i:i+chunk_size] for i in range(0, chunks, chunk_size) ]\n",
        "  with Pool() as p:\n",
        "    ret_list = p.map(partial(decompressChunk, dic=dic), string_list)\n",
        "  ret = ''.join(ret_list)\n",
        "  return ret"
      ],
      "metadata": {
        "id": "O7yA4Ya0Z8HT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next step on compression: Huffman\n",
        "\n",
        "from re import I\n",
        "import heapq\n",
        "from heapq import heappop, heappush, heapify\n",
        "\n",
        "class Huffman():\n",
        "  def __init__(self, ch, freq, left=None, right=None):\n",
        "    self.ch = ch\n",
        "    self.freq = freq\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    return self.freq < other.freq\n",
        "\n",
        "  def __str__(self):\n",
        "    return f'{self.ch}: {self.freq}'\n",
        "\n",
        "\n",
        "def print2DTree(root, space=0, LEVEL_SPACE=12):\n",
        "  if (root == None):\n",
        "    return\n",
        "  space += LEVEL_SPACE\n",
        "  print2DTree(root.right, space)\n",
        "  # print() # neighbor space\n",
        "  for i in range(LEVEL_SPACE, space):\n",
        "    print(end=\" \")\n",
        "  print(\"|\" + str(root) +\n",
        "      (\"|<\" if not(root.left is None and root.right is None) else \"\"))\n",
        "  print2DTree(root.left, space)\n",
        "\n",
        "\n",
        "def isLeaf(root):\n",
        "  return root.left is None and root.right is None\n",
        "\n",
        "\n",
        "def encode(root, s, code):\n",
        "  if root is None:\n",
        "    return\n",
        "  if isLeaf(root):\n",
        "    code[root.ch] = s if len(s) > 0 else '1'\n",
        "  encode(root.left, s + '0', code)\n",
        "  encode(root.right, s + '1', code)\n",
        "\n",
        "\n",
        "def encodeString(string, code):\n",
        "  s = ''\n",
        "  for i in range(0, len(string), 9):\n",
        "    s += code[string[i:i+9]]\n",
        "  return s\n",
        "\n",
        "\n",
        "def decode(root, i, s):\n",
        "  if root is None:\n",
        "    return i, ''\n",
        "  if isLeaf(root):\n",
        "    # print(root.ch, end='')\n",
        "    return i, root.ch\n",
        "  i += 1\n",
        "  root = root.left if s[i] == '0' else root.right\n",
        "  return decode(root, i, s)\n",
        "\n",
        "\n",
        "def getFreq(text):\n",
        "  s = set()\n",
        "  for i in range(0, len(text), 9):\n",
        "    s.add(text[i:i+9])\n",
        "  return {i: text.count(i) for i in s}\n",
        "\n",
        "\n",
        "def buildHuffmanTree(text, print_tree=False):\n",
        "  if len(text) == 0:\n",
        "    return\n",
        "  freq = getFreq(text)\n",
        "  pq = [Huffman(ch, freq[ch]) for ch in freq]\n",
        "  # pq.sort(reverse=True)\n",
        "  heapify(pq)\n",
        "  while len(pq) > 1:\n",
        "    left = heappop(pq)\n",
        "    right = heappop(pq)\n",
        "    total = left.freq + right.freq\n",
        "    heappush(pq, Huffman(None, total, left, right))\n",
        "\n",
        "  root = pq[0]\n",
        "  code = {}\n",
        "  encode(root, '', code)\n",
        "  if print_tree:\n",
        "    print2DTree(root)\n",
        "  return code, root\n",
        "\n",
        "def decodeString(string, hufTree):\n",
        "  decoded_string = ''\n",
        "  if isLeaf(hufTree):\n",
        "    decoded_string = hufTree.ch * len(string)\n",
        "  else:\n",
        "    i = -1\n",
        "    while i < len(string) - 1:\n",
        "      i, ch = decode(hufTree, i, string)\n",
        "      decoded_string += ch\n",
        "  return decoded_string\n",
        "\n"
      ],
      "metadata": {
        "id": "ObEZzMydxu7F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_binary(s):\n",
        "  while len(s)%8 != 0:\n",
        "    s += '0'\n",
        "  return bytearray(int(s[x:x+8], 2) for x in range(0, len(s), 8))"
      ],
      "metadata": {
        "id": "PSjjTzcxd-04"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir mycompressed"
      ],
      "metadata": {
        "id": "cQoJPf0FZeYQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tests sur la base d'apprentissage du dictionnaire**"
      ],
      "metadata": {
        "id": "XsyVEn-ZfJWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "NFiGcFVXrKTs"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First dictionnary not based on a book\n",
        "t1_1s = time.time()\n",
        "compressed = compress(text, tree, dic)\n",
        "\n",
        "# decompressed = decompress(compressed, reverted_dic)\n",
        "\n",
        "code, hufTree = buildHuffmanTree(compressed)\n",
        "\n",
        "compressed_2 = encodeString(compressed, code)\n",
        "t1_1e = time.time()\n",
        "\n",
        "t1_2s = time.time()\n",
        "compressed_parallel = compressMulti(text, tree, dic)\n",
        "\n",
        "code_parallel, hufTree_parallel = buildHuffmanTree(compressed_parallel)\n",
        "\n",
        "compressed_parallel_2 = encodeString(compressed_parallel, code_parallel)\n",
        "t1_2e = time.time()\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed)} bits puis {len(compressed_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed) / (len(text) * 8)} puis de {len(compressed_2) / (len(text) * 8)} après Huffman')\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed_parallel)} bits puis {len(compressed_parallel_2)} après Huffman pour la version parallele')\n",
        "print(f'Le taux de compression est donc de {len(compressed_parallel) / (len(text) * 8)} puis de {len(compressed_parallel_2) / (len(text) * 8)} après Huffman parallele')\n",
        "\n",
        "print(f'La compression basique a mis {round(t1_1e - t1_1s, 2)} secondes et la compression en parallele a mis {round(t1_2e - t1_2s, 2)} secondes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y394yPYxZrGi",
        "outputId": "942d2ea4-a34f-4eaa-d48f-adb50e20c69e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 5360544 bits puis 4023128 après Huffman\n",
            "Le taux de compression est donc de 0.9848582388994224 puis de 0.7391434072636948 après Huffman\n",
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 5360544 bits puis 4023128 après Huffman pour la version parallele\n",
            "Le taux de compression est donc de 0.9848582388994224 puis de 0.7391434072636948 après Huffman parallele\n",
            "La compression basique a mis 68.98 secondes et la compression en parallele a mis 28.08 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second dictionnary (made with only strings of 3 char)\n",
        "t2_1s = time.time()\n",
        "compressed2 = compress(text, tree2, dic2)\n",
        "\n",
        "# decompressed2 = decompress(compressed2, reverted_dic2)\n",
        "\n",
        "code2, hufTree2 = buildHuffmanTree(compressed2)\n",
        "\n",
        "compressed2_2 = encodeString(compressed2, code2)\n",
        "t2_1e = time.time()\n",
        "\n",
        "t2_2s = time.time()\n",
        "compressed_parallel2 = compressMulti(text, tree2, dic2)\n",
        "\n",
        "code_parallel2, hufTree_parallel2 = buildHuffmanTree(compressed_parallel2)\n",
        "\n",
        "compressed_parallel2_2 = encodeString(compressed_parallel2, code_parallel2)\n",
        "t2_2e = time.time()\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed2)} bits puis {len(compressed2_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed2) / (len(text) * 8)} puis de {len(compressed2_2) / (len(text) * 8)} après Huffman')\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed_parallel2)} bits puis {len(compressed_parallel2_2)} après Huffman pour la version parallele')\n",
        "print(f'Le taux de compression est donc de {len(compressed_parallel2) / (len(text) * 8)} puis de {len(compressed_parallel2_2) / (len(text) * 8)} après Huffman parallele')\n",
        "\n",
        "print(f'La compression basique a mis {round(t2_1e - t2_1s, 2)} secondes et la compression en parallele a mis {round(t2_2e - t2_2s, 2)} secondes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntOWF28ZyVg",
        "outputId": "cc4746ab-5115-40e7-cd3a-9544797b3224"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3422448 bits puis 3056526 après Huffman\n",
            "Le taux de compression est donc de 0.6287843379337713 puis de 0.56155584461396 après Huffman\n",
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3422448 bits puis 3056526 après Huffman pour la version parallele\n",
            "Le taux de compression est donc de 0.6287843379337713 puis de 0.56155584461396 après Huffman parallele\n",
            "La compression basique a mis 44.48 secondes et la compression en parallele a mis 20.48 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Third dictionnary (made with strings of 2-3 char)\n",
        "t3_1s = time.time()\n",
        "compressed3 = compress(text, tree3, dic3)\n",
        "\n",
        "# decompressed3 = decompress(compressed3, reverted_dic3)\n",
        "\n",
        "code3, hufTree3 = buildHuffmanTree(compressed3)\n",
        "\n",
        "compressed3_2 = encodeString(compressed3, code3)\n",
        "t3_1e = time.time()\n",
        "\n",
        "t3_2s = time.time()\n",
        "compressed_parallel3 = compressMulti(text, tree3, dic3)\n",
        "\n",
        "code_parallel3, hufTree_parallel3 = buildHuffmanTree(compressed_parallel3)\n",
        "\n",
        "compressed_parallel3_2 = encodeString(compressed_parallel3, code_parallel3)\n",
        "t3_2e = time.time()\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed3)} bits puis {len(compressed3_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed3) / (len(text) * 8)} puis de {len(compressed3_2) / (len(text) * 8)} après Huffman')\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed_parallel3)} bits puis {len(compressed_parallel3_2)} après Huffman pour la version parallele')\n",
        "print(f'Le taux de compression est donc de {len(compressed_parallel3) / (len(text) * 8)} puis de {len(compressed_parallel3_2) / (len(text) * 8)} après Huffman parallele')\n",
        "\n",
        "print(f'La compression basique a mis {round(t3_1e - t3_1s, 2)} secondes et la compression en parallele a mis {round(t3_2e - t3_2s, 2)} secondes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_g1-f2Yk27Z",
        "outputId": "d7d46596-b399-43d4-ff11-e0c008694262"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3052683 bits puis 2835925 après Huffman\n",
            "Le taux de compression est donc de 0.5608497949645046 puis de 0.5210262430736217 après Huffman\n",
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3052683 bits puis 2835925 après Huffman pour la version parallele\n",
            "Le taux de compression est donc de 0.5608497949645046 puis de 0.5210262430736217 après Huffman parallele\n",
            "La compression basique a mis 39.63 secondes et la compression en parallele a mis 18.67 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fourth dictionnary (made with only strings of 2-9 char)\n",
        "t4_1s = time.time()\n",
        "compressed4 = compress(text, tree4, dic4)\n",
        "\n",
        "# decompressed4 = decompress(compressed4, reverted_dic4)\n",
        "\n",
        "code4, hufTree4 = buildHuffmanTree(compressed4)\n",
        "\n",
        "compressed4_2 = encodeString(compressed4, code4)\n",
        "\n",
        "t4_1e = time.time()\n",
        "\n",
        "t4_2s = time.time()\n",
        "compressed_parallel4 = compressMulti(text, tree4, dic4)\n",
        "\n",
        "code_parallel4, hufTree_parallel4 = buildHuffmanTree(compressed_parallel4)\n",
        "\n",
        "compressed_parallel4_2 = encodeString(compressed_parallel4, code_parallel4)\n",
        "t4_2e = time.time()\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed4)} bits puis {len(compressed4_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed4) / (len(text) * 8)} puis de {len(compressed4_2) / (len(text) * 8)} après Huffman')\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed_parallel4)} bits puis {len(compressed_parallel4_2)} après Huffman pour la version parallele')\n",
        "print(f'Le taux de compression est donc de {len(compressed_parallel4) / (len(text) * 8)} puis de {len(compressed_parallel4_2) / (len(text) * 8)} après Huffman parallele')\n",
        "\n",
        "print(f'La compression basique a mis {round(t4_1e - t4_1s, 2)} secondes et la compression en parallele a mis {round(t4_2e - t4_2s, 2)} secondes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d1c9fe-e9ac-4bf4-b71d-0724f0e9e954",
        "id": "q-cJTpd9Rdn5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3031308 bits puis 2806662 après Huffman\n",
            "Le taux de compression est donc de 0.5569227038229199 puis de 0.5156499404735658 après Huffman\n",
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3031308 bits puis 2806662 après Huffman pour la version parallele\n",
            "Le taux de compression est donc de 0.5569227038229199 puis de 0.5156499404735658 après Huffman parallele\n",
            "La compression basique a mis 39.12 secondes et la compression en parallele a mis 18.5 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fifth dictionnary (made with only strings of 2-49 char)\n",
        "t5_1s = time.time()\n",
        "compressed5 = compress(text, tree5, dic5)\n",
        "\n",
        "# decompressed5 = decompress(compressed5, reverted_dic5)\n",
        "\n",
        "code5, hufTree5 = buildHuffmanTree(compressed5)\n",
        "\n",
        "compressed5_2 = encodeString(compressed5, code5)\n",
        "t5_1e = time.time()\n",
        "\n",
        "t5_2s = time.time()\n",
        "compressed_parallel5 = compressMulti(text, tree5, dic5)\n",
        "\n",
        "code_parallel5, hufTree_parallel5 = buildHuffmanTree(compressed_parallel5)\n",
        "\n",
        "compressed_parallel5_2 = encodeString(compressed_parallel5, code_parallel5)\n",
        "t5_2e = time.time()\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed5)} bits puis {len(compressed5_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed5) / (len(text) * 8)} puis de {len(compressed5_2) / (len(text) * 8)} après Huffman')\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed_parallel5)} bits puis {len(compressed_parallel5_2)} après Huffman pour la version parallele')\n",
        "print(f'Le taux de compression est donc de {len(compressed_parallel5) / (len(text) * 8)} puis de {len(compressed_parallel5_2) / (len(text) * 8)} après Huffman parallele')\n",
        "\n",
        "print(f'La compression basique a mis {round(t5_1e - t5_1s, 2)} secondes et la compression en parallele a mis {round(t5_2e - t5_2s, 2)} secondes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa84c597-9fc9-44bf-d2a6-0b4bb580eba9",
        "id": "M4mWLXmVReFY"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3031308 bits puis 2806662 après Huffman\n",
            "Le taux de compression est donc de 0.5569227038229199 puis de 0.5156499404735658 après Huffman\n",
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3031308 bits puis 2806662 après Huffman pour la version parallele\n",
            "Le taux de compression est donc de 0.5569227038229199 puis de 0.5156499404735658 après Huffman parallele\n",
            "La compression basique a mis 39.02 secondes et la compression en parallele a mis 18.48 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mycompressed/maupassant_bel_ami_illustre.cpsd', 'wb') as f:\n",
        "  f.write(get_binary(compressed_parallel5_2))"
      ],
      "metadata": {
        "id": "iIxwXoaOZirs"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test du dictionnaire sur un autre livre, ici le premier tome du livre \"Les misérables\"**"
      ],
      "metadata": {
        "id": "njuNcCPPTnWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text2 = textract.process(\"dataFrenchWords/hugo_les_miserables_fantine_source.doc\")\n",
        "text2 = text2.decode(\"utf-8\")\n",
        "text2 = text2.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "d3sqDrKnUFrb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t6_1s = time.time()\n",
        "compressed6 = compress(text2, tree5, dic5)\n",
        "\n",
        "# decompressed6 = decompress(compressed6, reverted_dic5)\n",
        "\n",
        "code6, hufTree6 = buildHuffmanTree(compressed6)\n",
        "\n",
        "compressed6_2 = encodeString(compressed6, code6)\n",
        "t6_1e = time.time()\n",
        "\n",
        "t6_2s = time.time()\n",
        "compressed_parallel6 = compressMulti(text2, tree5, dic5)\n",
        "\n",
        "code_parallel6, hufTree_parallel6 = buildHuffmanTree(compressed_parallel6)\n",
        "\n",
        "compressed_parallel6_2 = encodeString(compressed_parallel6, code_parallel6)\n",
        "t6_2e = time.time()\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text2) * 8} bits et celui compressé fait {len(compressed6)} bits puis {len(compressed6_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed6) / (len(text2) * 8)} puis de {len(compressed6_2) / (len(text2) * 8)} après Huffman')\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text2) * 8} bits et celui compressé fait {len(compressed_parallel6)} bits puis {len(compressed_parallel6_2)} après Huffman pour la version parallele')\n",
        "print(f'Le taux de compression est donc de {len(compressed_parallel6) / (len(text2) * 8)} puis de {len(compressed_parallel6_2) / (len(text2) * 8)} après Huffman parallele')\n",
        "\n",
        "print(f'La compression basique a mis {round(t6_1e - t6_1s, 2)} secondes et la compression en parallele a mis {round(t6_2e - t6_2s, 2)} secondes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItGmxIkvUMIn",
        "outputId": "6e8859ce-8f81-4b34-e8c8-7cf0d3d8dbbf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 6131592 bits et celui compressé fait 3502737 bits puis 3252788 après Huffman\n",
            "Le taux de compression est donc de 0.5712606122520872 puis de 0.5304964844366683 après Huffman\n",
            "Le fichier avant décompression faisait 6131592 bits et celui compressé fait 3502737 bits puis 3252788 après Huffman pour la version parallele\n",
            "Le taux de compression est donc de 0.5712606122520872 puis de 0.5304964844366683 après Huffman parallele\n",
            "La compression basique a mis 49.83 secondes et la compression en parallele a mis 22.79 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mycompressed/hugo_les_miserables_fantine_source.cpsd', 'w') as f:\n",
        "  f.write(compressed_parallel6_2)"
      ],
      "metadata": {
        "id": "m_bC4Dwwa6VH"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test avec un Roman de Science-fiction**"
      ],
      "metadata": {
        "id": "ShUwl7dLtM7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text3 = textract.process(\"dataFrenchWords/abbot_flatland_source.doc\")\n",
        "text3 = text3.decode(\"utf-8\")\n",
        "text3 = text3.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "eZEGPzmMtMSy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t7_1s = time.time()\n",
        "compressed7 = compress(text3, tree5, dic5)\n",
        "\n",
        "# decompressed7 = decompress(compressed7, reverted_dic5)\n",
        "\n",
        "code7, hufTree7 = buildHuffmanTree(compressed7)\n",
        "\n",
        "compressed7_2 = encodeString(compressed7, code7)\n",
        "t7_1e = time.time()\n",
        "\n",
        "t7_2s = time.time()\n",
        "compressed_parallel7 = compressMulti(text3, tree5, dic5)\n",
        "\n",
        "code_parallel7, hufTree_parallel7 = buildHuffmanTree(compressed_parallel7)\n",
        "\n",
        "compressed_parallel7_2 = encodeString(compressed_parallel7, code_parallel7)\n",
        "t7_2e = time.time()\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text3) * 8} bits et celui compressé fait {len(compressed7)} bits puis {len(compressed7_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed7) / (len(text3) * 8)} puis de {len(compressed7_2) / (len(text3) * 8)} après Huffman')\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text3) * 8} bits et celui compressé fait {len(compressed_parallel7)} bits puis {len(compressed_parallel7_2)} après Huffman pour la version parallele')\n",
        "print(f'Le taux de compression est donc de {len(compressed_parallel7) / (len(text3) * 8)} puis de {len(compressed_parallel7_2) / (len(text) * 8)} après Huffman parallele')\n",
        "\n",
        "print(f'La compression basique a mis {round(t7_1e - t7_1s, 2)} secondes et la compression en parallele a mis {round(t7_2e - t7_2s, 2)} secondes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGMug9PDuMFf",
        "outputId": "304e1d10-8b97-404d-8af1-a9fe6da06bb6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 1955288 bits et celui compressé fait 1122732 bits puis 1035390 après Huffman\n",
            "Le taux de compression est donc de 0.5742028795757965 puis de 0.5295332452303702 après Huffman\n",
            "Le fichier avant décompression faisait 1955288 bits et celui compressé fait 1122741 bits puis 1035445 après Huffman pour la version parallele\n",
            "Le taux de compression est donc de 0.5742074824782846 puis de 0.19023564384085132 après Huffman parallele\n",
            "La compression basique a mis 6.57 secondes et la compression en parallele a mis 3.99 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mycompressed/abbot_flatland_source.cpsd', 'wb') as f:\n",
        "  f.write(get_binary(compressed_parallel7_2))"
      ],
      "metadata": {
        "id": "lmYD94IDbMMJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test avec des Nouvelles - Contes Humour**"
      ],
      "metadata": {
        "id": "n0GxrW6t01kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text4 = textract.process(\"dataFrenchWords/allais_a_se_tordre_source.doc\")\n",
        "text4 = text4.decode(\"utf-8\")\n",
        "text4 = text4.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "QOCZ06PT01kU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t8_1s = time.time()\n",
        "compressed8 = compress(text4, tree5, dic5)\n",
        "\n",
        "# decompressed8 = decompress(compressed8, reverted_dic5)\n",
        "\n",
        "code8, hufTree8 = buildHuffmanTree(compressed8)\n",
        "\n",
        "compressed8_2 = encodeString(compressed8, code8)\n",
        "t8_1e = time.time()\n",
        "\n",
        "t8_2s = time.time()\n",
        "compressed_parallel8 = compressMulti(text4, tree5, dic5)\n",
        "\n",
        "code_parallel8, hufTree_parallel8 = buildHuffmanTree(compressed_parallel8)\n",
        "\n",
        "compressed_parallel8_2 = encodeString(compressed_parallel8, code_parallel8)\n",
        "t8_2e = time.time()\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text4) * 8} bits et celui compressé fait {len(compressed8)} bits puis {len(compressed8_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed8) / (len(text4) * 8)} puis de {len(compressed8_2) / (len(text4) * 8)} après Huffman')\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text4) * 8} bits et celui compressé fait {len(compressed_parallel8)} bits puis {len(compressed_parallel8_2)} après Huffman pour la version parallele')\n",
        "print(f'Le taux de compression est donc de {len(compressed_parallel8) / (len(text4) * 8)} puis de {len(compressed_parallel8_2) / (len(text4) * 8)} après Huffman parallele')\n",
        "\n",
        "print(f'La compression basique a mis {round(t8_1e - t8_1s, 2)} secondes et la compression en parallele a mis {round(t8_2e - t8_2s, 2)} secondes')"
      ],
      "metadata": {
        "outputId": "61b78e7e-cef3-4baa-dc3d-de3345d3f962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU-0UfhJ01kU"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 1935936 bits et celui compressé fait 1101645 bits puis 1015447 après Huffman\n",
            "Le taux de compression est donc de 0.5690503198452841 puis de 0.5245250876062019 après Huffman\n",
            "Le fichier avant décompression faisait 1935936 bits et celui compressé fait 1101654 bits puis 1015458 après Huffman pour la version parallele\n",
            "Le taux de compression est donc de 0.5690549687592978 puis de 0.5245307696122186 après Huffman parallele\n",
            "La compression basique a mis 6.31 secondes et la compression en parallele a mis 3.9 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mycompressed/allais_a_se_tordre_source.cpsd', 'wb') as f:\n",
        "  f.write(get_binary(compressed_parallel8_2))"
      ],
      "metadata": {
        "id": "qI5xOCI9bd5q"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test avec des Nouvelles - Contes Jeunesse**"
      ],
      "metadata": {
        "id": "ZdX1w3eZ2jPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text5 = textract.process(\"dataFrenchWords/andersen_contes_tome1_source.doc\")\n",
        "text5 = text5.decode(\"utf-8\")\n",
        "text5 = text5.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "4Vt0PtfM2jPp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t9_1s = time.time()\n",
        "compressed9 = compress(text5, tree5, dic5)\n",
        "\n",
        "decompressed9 = decompress(compressed9, reverted_dic5)\n",
        "\n",
        "code9, hufTree9 = buildHuffmanTree(compressed9)\n",
        "\n",
        "compressed9_2 = encodeString(compressed9, code9)\n",
        "t9_1e = time.time()\n",
        "\n",
        "t9_2s = time.time()\n",
        "compressed_parallel9 = compressMulti(text5, tree5, dic5)\n",
        "\n",
        "code_parallel9, hufTree_parallel9 = buildHuffmanTree(compressed_parallel9)\n",
        "\n",
        "compressed_parallel9_2 = encodeString(compressed_parallel9, code_parallel9)\n",
        "t9_2e = time.time()\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text5) * 8} bits et celui compressé fait {len(compressed9)} bits puis {len(compressed9_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed9) / (len(text5) * 8)} puis de {len(compressed9_2) / (len(text5) * 8)} après Huffman')\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text5) * 8} bits et celui compressé fait {len(compressed_parallel9)} bits puis {len(compressed_parallel9_2)} après Huffman pour la version parallele')\n",
        "print(f'Le taux de compression est donc de {len(compressed_parallel9) / (len(text5) * 8)} puis de {len(compressed_parallel9_2) / (len(text5) * 8)} après Huffman parallele')\n",
        "\n",
        "print(f'La compression basique a mis {round(t9_1e - t9_1s, 2)} secondes et la compression en parallele a mis {round(t9_2e - t9_2s, 2)} secondes')"
      ],
      "metadata": {
        "outputId": "d40b8187-08fa-4597-ed0a-f503dd96d8a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55ZLIF7u2jPp"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 3378528 bits et celui compressé fait 1883835 bits puis 1746725 après Huffman\n",
            "Le taux de compression est donc de 0.5575904654334669 puis de 0.51700770276286 après Huffman\n",
            "Le fichier avant décompression faisait 3378528 bits et celui compressé fait 1883835 bits puis 1746725 après Huffman pour la version parallele\n",
            "Le taux de compression est donc de 0.5575904654334669 puis de 0.51700770276286 après Huffman parallele\n",
            "La compression basique a mis 16.12 secondes et la compression en parallele a mis 8.51 secondes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mycompressed/anderson_contes_tome1_source.cpsd', 'wb') as f:\n",
        "  f.write(get_binary(compressed_parallel9_2))"
      ],
      "metadata": {
        "id": "rPMBwX2Pbk5L"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, platform, subprocess, re\n",
        "\n",
        "def get_processor_name():\n",
        "    if platform.system() == \"Windows\":\n",
        "        return platform.processor()\n",
        "    elif platform.system() == \"Darwin\":\n",
        "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin'\n",
        "        command =\"sysctl -n machdep.cpu.brand_string\"\n",
        "        return subprocess.check_output(command).strip()\n",
        "    elif platform.system() == \"Linux\":\n",
        "        command = \"cat /proc/cpuinfo\"\n",
        "        all_info = subprocess.check_output(command, shell=True).decode().strip()\n",
        "        for line in all_info.split(\"\\n\"):\n",
        "            if \"model name\" in line:\n",
        "                return re.sub( \".*model name.*:\", \"\", line,1)\n",
        "    return \"\"\n",
        "\n",
        "get_processor_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2JJFL4cQdf9M",
        "outputId": "01a65977-719b-435d-a3cc-ed7cc3bd9a0c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Intel(R) Xeon(R) CPU @ 2.20GHz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir notcompressed"
      ],
      "metadata": {
        "id": "L3rw17hXWUqD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir compressed"
      ],
      "metadata": {
        "id": "3c89gBgdWLk0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('notcompressed/abbot_flatland_source.txt', 'w') as f:\n",
        "  f.write(text3)\n",
        "\n",
        "with open('notcompressed/allais_a_se_tordre_source.txt', 'w') as f:\n",
        "  f.write(text4)\n",
        "\n",
        "with open('notcompressed/anderson_contes_tome1_source.txt', 'w') as f:\n",
        "  f.write(text5)\n",
        "\n",
        "with open('notcompressed/hugo_les_miserables_fantine_source.txt', 'w') as f:\n",
        "  f.write(text2)\n",
        "\n",
        "with open('notcompressed/bel_ami_illustre.txt', 'w') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "Q9ub5GbXWw5k"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tg1_s = time.time()\n",
        "!gzip -c notcompressed/abbot_flatland_source.txt > compressed/abbot_flatland_source.gz\n",
        "tg1_e = time.time()"
      ],
      "metadata": {
        "id": "L0rzmrNgWEtK"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'La compression de abbot_flatland_source a mis {round(tg1_e - tg1_s, 2)} secondes avec gzip et {round(t7_2e - t7_2s, 2)} avec l\\'algorithme personnalisé')\n",
        "print(f'La compression avec gzip prend {os.path.getsize(\"compressed/abbot_flatland_source.gz\")} bytes et celle de l\\'algorithme personnalisé prend {os.path.getsize(\"mycompressed/abbot_flatland_source.cpsd\")} bytes avec une taille initiale de {os.path.getsize(\"notcompressed/abbot_flatland_source.txt\")} bytes')"
      ],
      "metadata": {
        "id": "7_y6i_hGf6DW",
        "outputId": "7999a1b1-f425-420e-a667-c7230fc66051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La compression de abbot_flatland_source a mis 0.12 secondes avec gzip et 3.99 avec l'algorithme personnalisé\n",
            "La compression avec gzip prend 94329 bytes et celle de l'algorithme personnalisé prend 129431 bytes avec une taille initiale de 253056 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tg2_s = time.time()\n",
        "!gzip -c notcompressed/allais_a_se_tordre_source.txt > compressed/allais_a_se_tordre_source.gz\n",
        "tg2_e = time.time()"
      ],
      "metadata": {
        "id": "6pKm9uEjfUfx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tg3_s = time.time()\n",
        "!gzip -c notcompressed/anderson_contes_tome1_source.txt > compressed/andersen_contes_tome1_source.gz\n",
        "tg3_e = time.time()"
      ],
      "metadata": {
        "id": "FsMFLJOjfezb"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tg4_s = time.time()\n",
        "!gzip -c notcompressed/bel_ami_illustre.txt > compressed/bel_ami_illustre.gz\n",
        "tg4_e = time.time()"
      ],
      "metadata": {
        "id": "1DFcq-XwfluS"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tg5_s = time.time()\n",
        "!gzip -c notcompressed/hugo_les_miserables_fantine_source.txt > compressed/hugo_les_miserables_fantine_source.gz\n",
        "tg5_e = time.time()"
      ],
      "metadata": {
        "id": "Gmb7J4lKfrto"
      },
      "execution_count": 66,
      "outputs": []
    }
  ]
}