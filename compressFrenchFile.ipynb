{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPg6iBdyJM00VVXMgXCdYKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deRockerTom/dataFrenchWords/blob/main/compressFrenchFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/deRockerTom/dataFrenchWords\n",
        "!pip install textract\n",
        "!sudo apt-get install antiword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "scT0RXv3qF9z",
        "outputId": "db7ad700-0acf-44ad-c90e-b85f8de3d276"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dataFrenchWords'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 95 (delta 7), reused 0 (delta 0), pack-reused 78\u001b[K\n",
            "Unpacking objects: 100% (95/95), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting docx2txt~=0.8\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Collecting xlrd~=1.2.0\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six==20191110\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 17.0 MB/s \n",
            "\u001b[?25hCollecting SpeechRecognition~=3.8.1\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting python-pptx~=0.6.18\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 52.9 MB/s \n",
            "\u001b[?25hCollecting argcomplete~=1.10.0\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting beautifulsoup4~=4.8.0\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 70.5 MB/s \n",
            "\u001b[?25hCollecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from textract) (3.0.4)\n",
            "Collecting extract-msg<=0.29.*\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.3.2.post1)\n",
            "Collecting tzlocal>=2.1\n",
            "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
            "Collecting compressed-rtf>=1.0.6\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "Collecting ebcdic>=1.1.1\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 54.9 MB/s \n",
            "\u001b[?25hCollecting imapclient==2.1.0\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 71.0 MB/s \n",
            "\u001b[?25hCollecting pytz-deprecation-shim\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting tzdata\n",
            "  Downloading tzdata-2022.1-py2.py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 73.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile, python-pptx\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=631d2231ab0e1d88351233de9b56c5ea2807509a0d3433600cd628b890bb964b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6204 sha256=bcb27c10c815a757c386f8793ed27b3945174e2449374aed37d93d54b605b97d\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/33/88/88ceee84d1b74b391c086bc594d3fcf80800decfbd6e1ff565\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=db02e49b107e8419ec2dbd65606dd03c500910d6f81aaae9085a737123cb0634\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/53/e6/37d90ccb3ad1a3ca98d2b17107e9fda401a7c541ea1eb6a65a\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470951 sha256=aadbbcc43aae7da24ae4c7f9660bf75d46d54e9b466693cf455d5ece6efbc46e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/ab/f4/52560d0d4bd4055e9261c6df6e51c7b56c2b23cca3dee811a3\n",
            "Successfully built docx2txt compressed-rtf olefile python-pptx\n",
            "Installing collected packages: tzdata, backports.zoneinfo, six, pytz-deprecation-shim, XlsxWriter, tzlocal, pycryptodome, olefile, imapclient, ebcdic, compressed-rtf, xlrd, SpeechRecognition, python-pptx, pdfminer.six, extract-msg, docx2txt, beautifulsoup4, argcomplete, textract\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 1.5.1\n",
            "    Uninstalling tzlocal-1.5.1:\n",
            "      Successfully uninstalled tzlocal-1.5.1\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.31.6 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.3 argcomplete-1.10.3 backports.zoneinfo-0.2.1 beautifulsoup4-4.8.2 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.15.0 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 six-1.12.0 textract-1.6.5 tzdata-2022.1 tzlocal-4.2 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  antiword\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 128 kB of archives.\n",
            "After this operation, 633 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 antiword amd64 0.37-11build1 [128 kB]\n",
            "Fetched 128 kB in 1s (181 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package antiword.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../antiword_0.37-11build1_amd64.deb ...\n",
            "Unpacking antiword (0.37-11build1) ...\n",
            "Setting up antiword (0.37-11build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text = textract.process(\"dataFrenchWords/maupassant_bel_ami_illustre.doc\")\n",
        "text = text.decode(\"utf-8\")\n",
        "text = text.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "m-h39iUgJbvv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dict with file\n",
        "import os\n",
        "\n",
        "def getDictWithFile(file):\n",
        "  with open(file, 'r') as f:\n",
        "    dic = {}\n",
        "    key = \"\"\n",
        "    value = \"\"\n",
        "    isKey = True\n",
        "    for l in f.readlines():\n",
        "      for c in l:\n",
        "        if c == 'ក':\n",
        "          isKey = False\n",
        "        elif c == 'ខ':\n",
        "          isKey = True\n",
        "          dic[key] = int(value)\n",
        "          key = \"\"\n",
        "          value = \"\"\n",
        "        else:\n",
        "          if isKey:\n",
        "            key = key + c\n",
        "          else:\n",
        "            value = value + c\n",
        "    value = value + \"\\n\"\n",
        "  return dic\n",
        "\n",
        "dicByHugo = getDictWithFile('dataFrenchWords/dicByHugo')\n",
        "dicByHugoMax = getDictWithFile('dataFrenchWords/dicByHugoMax')\n",
        "dictByHugoMax10 = getDictWithFile('dataFrenchWords/dicByHugoMax10')\n",
        "dictByHugoMax50 = getDictWithFile('dataFrenchWords/dicByHugoMax50')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D-79WIkptQUQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertToBinary(number):\n",
        "  binary = ''\n",
        "  while number > 0:\n",
        "    binary = str(number % 2) + binary\n",
        "    number = number // 2\n",
        "  return binary\n",
        "def convertToNineBits(bits):\n",
        "  n = len(bits)\n",
        "  if n < 9:\n",
        "    k = 9 - n\n",
        "    return str('0') * k + bits\n",
        "  elif n == 9:\n",
        "    return bits\n",
        "  else:\n",
        "    print(\"Attention : un des encodages est trop grand (>9)\")\n",
        "    print(bits)\n",
        "\n",
        "convertToNineBits(convertToBinary(255))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3osK_tKk8uSu",
        "outputId": "cd3af23f-822f-4afc-b6b4-24e59777d0a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'011111111'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# open csv\n",
        "dic = {}\n",
        "with open('dataFrenchWords/french-word-list-verbs.csv', 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  #drop first 3 lines\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  #create dictionary\n",
        "  for row in reader:\n",
        "    if row[1] in dic:\n",
        "      dic[row[1]] += int(row[2])\n",
        "    else:\n",
        "      dic[row[1]] = int(row[2])\n",
        "  csvfile.close()\n",
        "\n",
        "with open('dataFrenchWords/french-word-list-adjectives.csv', 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  #drop first 3 lines\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  #create dictionary\n",
        "  for row in reader:\n",
        "    if row[1] in dic:\n",
        "      dic[row[1]] += int(row[2])\n",
        "    else:\n",
        "      dic[row[1]] = int(row[2])\n",
        "  csvfile.close()\n",
        "\n",
        "with open('dataFrenchWords/french-word-list-nouns.csv', 'r') as csvfile:\n",
        "  reader = csv.reader(csvfile)\n",
        "  #drop first 3 lines\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  next(reader)\n",
        "  #create dictionary\n",
        "  for row in reader:\n",
        "    if row[1] in dic:\n",
        "      dic[row[1]] += int(row[2])\n",
        "    else:\n",
        "      dic[row[1]] = int(row[2])\n",
        "  csvfile.close()\n",
        "\n",
        "# Ponctuation :\n",
        "\n",
        "dic[', '] = 150000\n",
        "dic['. '] = 150000\n",
        "\n",
        "dic = dict(sorted(dic.items(), key=lambda item: item[1], reverse=True))\n",
        "dic = {k: dic[k] for k in list(dic)[:256]} # In order to code the dictionnary on only 2 other bytes with most frequent words\n",
        "offset = 256\n",
        "i = 0\n",
        "for key in dic:\n",
        "  dic[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "dic2 = {}\n",
        "offset = 256\n",
        "for key in dicByHugo:\n",
        "  dic2[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "dic3 = {}\n",
        "offset = 256\n",
        "for key in dicByHugoMax:\n",
        "  dic3[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "dic4 = {}\n",
        "offset = 256\n",
        "for key in dictByHugoMax10:\n",
        "  dic4[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "dic5 = {}\n",
        "offset = 256\n",
        "for key in dictByHugoMax50:\n",
        "  dic5[key] = convertToNineBits(convertToBinary(offset + i))\n",
        "  offset += 1\n",
        "\n",
        "print(dic5)\n",
        "\n",
        "reverted_dic = {v: k for k, v in dic.items()}\n",
        "reverted_dic2 = {v: k for k, v in dic2.items()}\n",
        "reverted_dic3 = {v: k for k, v in dic3.items()}\n",
        "reverted_dic4 = {v: k for k, v in dic4.items()}\n",
        "reverted_dic5 = {v: k for k, v in dic5.items()}\n",
        "\n",
        "print(reverted_dic5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awJCxj_ArQG9",
        "outputId": "28995be7-daaf-4de2-ce31-531472896806"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'  ': '100000000', 'e ': '100000001', 's ': '100000010', 't ': '100000011', ' d': '100000100', ', ': '100000101', 'ai': '100000110', 'es': '100000111', ' l': '100001000', 'le': '100001001', 'en': '100001010', 're': '100001011', 'ou': '100001100', 'de': '100001101', ' s': '100001110', 'nt': '100001111', 'it': '100010000', ' p': '100010001', 'on': '100010010', ' c': '100010011', 'an': '100010100', 'n ': '100010101', ' e': '100010110', 'ur': '100010111', 'te': '100011000', 'er': '100011001', 'is': '100011010', 'r ': '100011011', '\\n ': '100011100', ' a': '100011101', '\\n\\n': '100011110', 'a ': '100011111', 'qu': '100100000', '   ': '100100001', 'se': '100100010', 'ne': '100100011', 'et': '100100100', ' m': '100100101', 'me': '100100110', 'la': '100100111', ' de': '100101000', 'ie': '100101001', 'il': '100101010', 'e  ': '100101011', 'eu': '100101100', ' t': '100101101', 'ra': '100101110', 'ar': '100101111', 'in': '100110000', 'll': '100110001', 'ui': '100110010', 'l ': '100110011', 'es ': '100110100', 'un': '100110101', '. ': '100110110', '\\n  ': '100110111', ' f': '100111000', 'us': '100111001', '\\n\\n ': '100111010', '\\n\\n\\n': '100111011', 'co': '100111100', ' q': '100111101', ' r': '100111110', ' v': '100111111', 'tr': '101000000', 'pa': '101000001', 'ns': '101000010', 'ue': '101000011', 'it ': '101000100', 'oi': '101000101', 'ait': '101000110', 'e,': '101000111', '    ': '101001000', 'ce': '101001001', 'sa': '101001010', 'de ': '101001011', 've': '101001100', 'le ': '101001101', 'i ': '101001110', 'ta': '101001111', 'ut': '101010000', 'ri': '101010001', 'au': '101010010', 'u ': '101010011', 'ch': '101010100', 'em': '101010101', 'nd': '101010110', 's  ': '101010111', 'ti': '101011000', 'ir': '101011001', ' le': '101011010', '\\n   ': '101011011', 'ent': '101011100', 'ma': '101011101', 'ro': '101011110', '\\n\\n  ': '101011111', 'so': '101100000', 'po': '101100001', '\\n\\n\\n ': '101100010', ' u': '101100011', 'nt ': '101100100', 'el': '101100101', 't  ': '101100110', ' n': '101100111', 'or': '101101000', 'ss': '101101001', ' i': '101101010', ' b': '101101011', 's,': '101101100', ' qu': '101101101', '  d': '101101110', '.\\n': '101101111', 'om': '101110000', \"'a\": '101110001', 'av': '101110010', 'as': '101110011', 'vo': '101110100', 'va': '101110101', '\\n    ': '101110110', 'à ': '101110111', 'st': '101111000', '\\n\\n   ': '101111001', '\\n\\n\\n  ': '101111010', 'lle': '101111011', 'e.': '101111100', 'pr': '101111101', 'to': '101111110', '«\\xa0': '101111111', ' à': '110000000', ' j': '110000001', 'ne ': '110000010', 'da': '110000011', ' «': '110000100', '\\xa0»': '110000101', 'e, ': '110000110', ' de ': '110000111', ',  ': '110001000', 'al': '110001001', 'lu': '110001010', 'si': '110001011', 'ant': '110001100', ' et': '110001101', 'et ': '110001110', 'mm': '110001111', 'pe': '110010000', '\\xa0:': '110010001', 'ait ': '110010010', 'e\\n': '110010011', ' la': '110010100', '  l': '110010101', 'mo': '110010110', '\\n\\n    ': '110010111', '\\n\\n\\n   ': '110011000', ' pa': '110011001', ' un': '110011010', 'la ': '110011011', 't,': '110011100', 'di': '110011101', \"l'\": '110011110', 'he': '110011111', 'Il': '110100000', 'our': '110100001', '»\\n': '110100010', 'rt': '110100011', \"d'\": '110100100', 'que': '110100101', ' se': '110100110', 're ': '110100111', \"'e\": '110101000', ' I': '110101001', ' co': '110101010', 'na': '110101011', 'ux': '110101100', '\\n\\n\\n    ': '110101101', '.\\xa0': '110101110', 's, ': '110101111', 'e d': '110110000', 'ré': '110110001', 'nc': '110110010', 'rs': '110110011', 'ais': '110110100', 'les': '110110101', 'is ': '110110110', 'mi': '110110111', 's.': '110111000', '  s': '110111001', 'il ': '110111010', 'ge': '110111011', 'on ': '110111100', 'li': '110111101', 'su': '110111110', 'ét': '110111111', 'ur ': '111000000', 'fa': '111000001', '  p': '111000010', 'at': '111000011', '.\\n\\n': '111000100', ' g': '111000101', ' «\\xa0': '111000110', ' E': '111000111', ' so': '111001000', ':\\n': '111001001', ' en': '111001010', 'er ': '111001011', 'vi': '111001100', 'ous': '111001101', 'ns ': '111001110', 'us ': '111001111', 'ien': '111010000', ' à ': '111010001', 'e s': '111010010', 'ec': '111010011', ' et ': '111010100', 'ha': '111010101', 'tt': '111010110', 'e\\xa0': '111010111', 'x ': '111011000', 's\\n': '111011001', '  c': '111011010', 'ée': '111011011', ' é': '111011100', 'é ': '111011101', 'une': '111011110', 'en ': '111011111', ' po': '111100000', 'un ': '111100001', \"u'\": '111100010', 'pl': '111100011', 'e l': '111100100', ' il': '111100101', '  e': '111100110', 'mme': '111100111', 'n  ': '111101000', ' o': '111101001', ' la ': '111101010', 'no': '111101011', 'te ': '111101100', 'uv': '111101101', 'ue ': '111101110', 'nn': '111101111', 'me ': '111110000', ' re': '111110001', 'je': '111110010', 'tre': '111110011', 'bl': '111110100', '\\xa0»\\n': '111110101', 'du': '111110110', '»\\n\\n': '111110111', 't d': '111111000', '  «': '111111001', 't, ': '111111010', 'lo': '111111011', 'eur': '111111100', 'ui ': '111111101', ' Il': '111111110', ' vo': '111111111'}\n",
            "{'100000000': '  ', '100000001': 'e ', '100000010': 's ', '100000011': 't ', '100000100': ' d', '100000101': ', ', '100000110': 'ai', '100000111': 'es', '100001000': ' l', '100001001': 'le', '100001010': 'en', '100001011': 're', '100001100': 'ou', '100001101': 'de', '100001110': ' s', '100001111': 'nt', '100010000': 'it', '100010001': ' p', '100010010': 'on', '100010011': ' c', '100010100': 'an', '100010101': 'n ', '100010110': ' e', '100010111': 'ur', '100011000': 'te', '100011001': 'er', '100011010': 'is', '100011011': 'r ', '100011100': '\\n ', '100011101': ' a', '100011110': '\\n\\n', '100011111': 'a ', '100100000': 'qu', '100100001': '   ', '100100010': 'se', '100100011': 'ne', '100100100': 'et', '100100101': ' m', '100100110': 'me', '100100111': 'la', '100101000': ' de', '100101001': 'ie', '100101010': 'il', '100101011': 'e  ', '100101100': 'eu', '100101101': ' t', '100101110': 'ra', '100101111': 'ar', '100110000': 'in', '100110001': 'll', '100110010': 'ui', '100110011': 'l ', '100110100': 'es ', '100110101': 'un', '100110110': '. ', '100110111': '\\n  ', '100111000': ' f', '100111001': 'us', '100111010': '\\n\\n ', '100111011': '\\n\\n\\n', '100111100': 'co', '100111101': ' q', '100111110': ' r', '100111111': ' v', '101000000': 'tr', '101000001': 'pa', '101000010': 'ns', '101000011': 'ue', '101000100': 'it ', '101000101': 'oi', '101000110': 'ait', '101000111': 'e,', '101001000': '    ', '101001001': 'ce', '101001010': 'sa', '101001011': 'de ', '101001100': 've', '101001101': 'le ', '101001110': 'i ', '101001111': 'ta', '101010000': 'ut', '101010001': 'ri', '101010010': 'au', '101010011': 'u ', '101010100': 'ch', '101010101': 'em', '101010110': 'nd', '101010111': 's  ', '101011000': 'ti', '101011001': 'ir', '101011010': ' le', '101011011': '\\n   ', '101011100': 'ent', '101011101': 'ma', '101011110': 'ro', '101011111': '\\n\\n  ', '101100000': 'so', '101100001': 'po', '101100010': '\\n\\n\\n ', '101100011': ' u', '101100100': 'nt ', '101100101': 'el', '101100110': 't  ', '101100111': ' n', '101101000': 'or', '101101001': 'ss', '101101010': ' i', '101101011': ' b', '101101100': 's,', '101101101': ' qu', '101101110': '  d', '101101111': '.\\n', '101110000': 'om', '101110001': \"'a\", '101110010': 'av', '101110011': 'as', '101110100': 'vo', '101110101': 'va', '101110110': '\\n    ', '101110111': 'à ', '101111000': 'st', '101111001': '\\n\\n   ', '101111010': '\\n\\n\\n  ', '101111011': 'lle', '101111100': 'e.', '101111101': 'pr', '101111110': 'to', '101111111': '«\\xa0', '110000000': ' à', '110000001': ' j', '110000010': 'ne ', '110000011': 'da', '110000100': ' «', '110000101': '\\xa0»', '110000110': 'e, ', '110000111': ' de ', '110001000': ',  ', '110001001': 'al', '110001010': 'lu', '110001011': 'si', '110001100': 'ant', '110001101': ' et', '110001110': 'et ', '110001111': 'mm', '110010000': 'pe', '110010001': '\\xa0:', '110010010': 'ait ', '110010011': 'e\\n', '110010100': ' la', '110010101': '  l', '110010110': 'mo', '110010111': '\\n\\n    ', '110011000': '\\n\\n\\n   ', '110011001': ' pa', '110011010': ' un', '110011011': 'la ', '110011100': 't,', '110011101': 'di', '110011110': \"l'\", '110011111': 'he', '110100000': 'Il', '110100001': 'our', '110100010': '»\\n', '110100011': 'rt', '110100100': \"d'\", '110100101': 'que', '110100110': ' se', '110100111': 're ', '110101000': \"'e\", '110101001': ' I', '110101010': ' co', '110101011': 'na', '110101100': 'ux', '110101101': '\\n\\n\\n    ', '110101110': '.\\xa0', '110101111': 's, ', '110110000': 'e d', '110110001': 'ré', '110110010': 'nc', '110110011': 'rs', '110110100': 'ais', '110110101': 'les', '110110110': 'is ', '110110111': 'mi', '110111000': 's.', '110111001': '  s', '110111010': 'il ', '110111011': 'ge', '110111100': 'on ', '110111101': 'li', '110111110': 'su', '110111111': 'ét', '111000000': 'ur ', '111000001': 'fa', '111000010': '  p', '111000011': 'at', '111000100': '.\\n\\n', '111000101': ' g', '111000110': ' «\\xa0', '111000111': ' E', '111001000': ' so', '111001001': ':\\n', '111001010': ' en', '111001011': 'er ', '111001100': 'vi', '111001101': 'ous', '111001110': 'ns ', '111001111': 'us ', '111010000': 'ien', '111010001': ' à ', '111010010': 'e s', '111010011': 'ec', '111010100': ' et ', '111010101': 'ha', '111010110': 'tt', '111010111': 'e\\xa0', '111011000': 'x ', '111011001': 's\\n', '111011010': '  c', '111011011': 'ée', '111011100': ' é', '111011101': 'é ', '111011110': 'une', '111011111': 'en ', '111100000': ' po', '111100001': 'un ', '111100010': \"u'\", '111100011': 'pl', '111100100': 'e l', '111100101': ' il', '111100110': '  e', '111100111': 'mme', '111101000': 'n  ', '111101001': ' o', '111101010': ' la ', '111101011': 'no', '111101100': 'te ', '111101101': 'uv', '111101110': 'ue ', '111101111': 'nn', '111110000': 'me ', '111110001': ' re', '111110010': 'je', '111110011': 'tre', '111110100': 'bl', '111110101': '\\xa0»\\n', '111110110': 'du', '111110111': '»\\n\\n', '111111000': 't d', '111111001': '  «', '111111010': 't, ', '111111011': 'lo', '111111100': 'eur', '111111101': 'ui ', '111111110': ' Il', '111111111': ' vo'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class to build a tree defining all the possible  string in dic\n",
        "class SuffixTree():\n",
        "  class Node():\n",
        "    def __init__(self,char):\n",
        "      self.char = char\n",
        "      self.out = {}\n",
        "      self.isFinal = False\n",
        "    def __str__(self, level=0):\n",
        "      ret = \"  \" * level + str(self.char) + \"\\n\"\n",
        "      for c in self.out:\n",
        "        ret += self.out[c].__str__(level + 1)\n",
        "      return ret\n",
        "\n",
        "  def __init__(self, dic):\n",
        "    self.root = self.Node(None)\n",
        "    self.root.isFinal = True\n",
        "    for key in dic:\n",
        "      cursor = self.root\n",
        "      for c in key:\n",
        "        if c in cursor.out:\n",
        "          cursor = cursor.out[c]\n",
        "        else:\n",
        "          node = self.Node(c)\n",
        "          cursor.out[c] = node\n",
        "          cursor = node\n",
        "      cursor.isFinal = True\n",
        "  def longestPrefix(self, s):\n",
        "    cursor = self.root\n",
        "    word = ''\n",
        "    temp = ''\n",
        "    end = False\n",
        "    i = 0\n",
        "    length = len(s)\n",
        "    while not(end) and i < length:\n",
        "      if s[i] in cursor.out:\n",
        "        temp = temp + s[i]\n",
        "        cursor = cursor.out[s[i]]\n",
        "        if cursor.isFinal:\n",
        "          word = temp\n",
        "        i += 1\n",
        "      else:\n",
        "        end = True\n",
        "    return word\n",
        "  def __str__(self):\n",
        "    return f'{self.root}'\n",
        "\n",
        "s = SuffixTree({'ab' : 1, 'abc': 2, 'coucou' : 5, 'aba' : 4, 'aaa' : 8})\n",
        "print(s)\n",
        "s.longestPrefix('abdc')\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "ND_fSFhFq51O",
        "outputId": "8df61625-21ae-4a6e-cda8-2fc83a77643e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "  a\n",
            "    b\n",
            "      c\n",
            "      a\n",
            "    a\n",
            "      a\n",
            "  c\n",
            "    o\n",
            "      u\n",
            "        c\n",
            "          o\n",
            "            u\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ab'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree = SuffixTree(dic)\n",
        "tree2 = SuffixTree(dic2)\n",
        "tree3 = SuffixTree(dic3)\n",
        "tree4 = SuffixTree(dic4)\n",
        "tree5 = SuffixTree(dic5)"
      ],
      "metadata": {
        "id": "FgOeftRxAILs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compress(string, tree, dic):\n",
        "  i = 0\n",
        "  ret = ''\n",
        "  while i<len(string):\n",
        "    temp = tree.longestPrefix(string[i:])\n",
        "    n = len(temp)\n",
        "    if n == 0:\n",
        "      bit = convertToNineBits(convertToBinary(ord(string[i])))\n",
        "      ret += bit\n",
        "      i += 1\n",
        "    else:\n",
        "      bit = dic[temp]\n",
        "      ret += bit\n",
        "      i += n\n",
        "  return ret\n",
        "\n",
        "\n",
        "\n",
        "def decompress(string, dic):\n",
        "  ret = ''\n",
        "  for i in range(0, len(string) - 1, 9):\n",
        "    if int(string[i : i + 9], 2) > 255:\n",
        "      ret += dic[string[i : i + 9]]\n",
        "    else:\n",
        "      ret += chr(int(string[i : i + 9], 2))\n",
        "  return ret"
      ],
      "metadata": {
        "id": "d22Anv76pQBY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi process version\n",
        "\n",
        "import multiprocessing\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "\n",
        "multiprocessing.cpu_count()\n",
        "\n",
        "def compressChunk(string, tree, dic):\n",
        "  i = 0\n",
        "  ret = ''\n",
        "  while i<len(string):\n",
        "    temp = tree.longestPrefix(string[i:])\n",
        "    n = len(temp)\n",
        "    if n == 0:\n",
        "      bit = convertToNineBits(convertToBinary(ord(string[i])))\n",
        "      ret += bit\n",
        "      i += 1\n",
        "    else:\n",
        "      bit = dic[temp]\n",
        "      ret += bit\n",
        "      i += n\n",
        "  return ret\n",
        "\n",
        "def compressMulti(string, tree, dic):\n",
        "  chunks, chunk_size = len(string), len(string)//4\n",
        "  string_list = [ string[i:i+chunk_size] for i in range(0, chunks, chunk_size) ]\n",
        "  with Pool() as p:\n",
        "    ret_list = p.map(partial(string_list, tree=tree, dic=dic), string_list)\n",
        "  ret = ''.join(ret_list)\n",
        "  return ret\n",
        "\n",
        "def decompressChunk(string, dic):\n",
        "  ret = ''\n",
        "  for i in range(0, len(string) - 1, 9):\n",
        "    if int(string[i : i + 9], 2) > 255:\n",
        "      ret += dic[string[i : i + 9]]\n",
        "    else:\n",
        "      ret += chr(int(string[i : i + 9], 2))\n",
        "  return ret\n",
        "\n",
        "def decompressMulti(string, dic):\n",
        "  ret = ''\n",
        "  for i in range(0, len(string) - 1, 9):\n",
        "    if int(string[i : i + 9], 2) > 255:\n",
        "      ret += dic[string[i : i + 9]]\n",
        "    else:\n",
        "      ret += chr(int(string[i : i + 9], 2))\n",
        "  return ret"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7yA4Ya0Z8HT",
        "outputId": "2535847a-0d93-4685-9638-8ca3ecbb8cec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "\n",
        "\n",
        "cc_cores_per_SM_dict = {\n",
        "    (2,0) : 32,\n",
        "    (2,1) : 48,\n",
        "    (3,0) : 192,\n",
        "    (3,5) : 192,\n",
        "    (3,7) : 192,\n",
        "    (5,0) : 128,\n",
        "    (5,2) : 128,\n",
        "    (6,0) : 64,\n",
        "    (6,1) : 128,\n",
        "    (7,0) : 64,\n",
        "    (7,5) : 64,\n",
        "    (8,0) : 64,\n",
        "    (8,6) : 128\n",
        "    }\n",
        "# the above dictionary should result in a value of \"None\" if a cc match \n",
        "# is not found.  The dictionary needs to be extended as new devices become\n",
        "# available, and currently does not account for all Jetson devices\n",
        "device = cuda.get_current_device()\n",
        "my_sms = getattr(device, 'MULTIPROCESSOR_COUNT')\n",
        "my_cc = device.compute_capability\n",
        "cores_per_sm = cc_cores_per_SM_dict.get(my_cc)\n",
        "total_cores = cores_per_sm*my_sms\n",
        "print(\"GPU compute capability: \" , my_cc)\n",
        "print(\"GPU total number of SMs: \" , my_sms)\n",
        "print(\"total cores: \" , total_cores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3cmKFbNaqt2",
        "outputId": "38b76a02-b66d-497f-9571-27cae98ead51"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU compute capability:  (7, 5)\n",
            "GPU total number of SMs:  40\n",
            "total cores:  2560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next step on compression: Huffman\n",
        "\n",
        "from re import I\n",
        "import heapq\n",
        "from heapq import heappop, heappush, heapify\n",
        "\n",
        "class Huffman():\n",
        "  def __init__(self, ch, freq, left=None, right=None):\n",
        "    self.ch = ch\n",
        "    self.freq = freq\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "\n",
        "  def __lt__(self, other):\n",
        "    return self.freq < other.freq\n",
        "\n",
        "  def __str__(self):\n",
        "    return f'{self.ch}: {self.freq}'\n",
        "\n",
        "\n",
        "def print2DTree(root, space=0, LEVEL_SPACE=12):\n",
        "  if (root == None):\n",
        "    return\n",
        "  space += LEVEL_SPACE\n",
        "  print2DTree(root.right, space)\n",
        "  # print() # neighbor space\n",
        "  for i in range(LEVEL_SPACE, space):\n",
        "    print(end=\" \")\n",
        "  print(\"|\" + str(root) +\n",
        "      (\"|<\" if not(root.left is None and root.right is None) else \"\"))\n",
        "  print2DTree(root.left, space)\n",
        "\n",
        "\n",
        "def isLeaf(root):\n",
        "  return root.left is None and root.right is None\n",
        "\n",
        "\n",
        "def encode(root, s, code):\n",
        "  if root is None:\n",
        "    return\n",
        "  if isLeaf(root):\n",
        "    code[root.ch] = s if len(s) > 0 else '1'\n",
        "  encode(root.left, s + '0', code)\n",
        "  encode(root.right, s + '1', code)\n",
        "\n",
        "\n",
        "def encodeString(string, code):\n",
        "  s = ''\n",
        "  for i in range(0, len(string), 9):\n",
        "    s += code[string[i:i+9]]\n",
        "  return s\n",
        "\n",
        "\n",
        "def decode(root, i, s):\n",
        "  if root is None:\n",
        "    return i, ''\n",
        "  if isLeaf(root):\n",
        "    # print(root.ch, end='')\n",
        "    return i, root.ch\n",
        "  i += 1\n",
        "  root = root.left if s[i] == '0' else root.right\n",
        "  return decode(root, i, s)\n",
        "\n",
        "\n",
        "def getFreq(text):\n",
        "  s = set()\n",
        "  for i in range(0, len(text), 9):\n",
        "    s.add(text[i:i+9])\n",
        "  return {i: text.count(i) for i in s}\n",
        "\n",
        "\n",
        "def buildHuffmanTree(text, print_tree=False):\n",
        "  if len(text) == 0:\n",
        "    return\n",
        "  freq = getFreq(text)\n",
        "  pq = [Huffman(ch, freq[ch]) for ch in freq]\n",
        "  # pq.sort(reverse=True)\n",
        "  heapify(pq)\n",
        "  while len(pq) > 1:\n",
        "    left = heappop(pq)\n",
        "    right = heappop(pq)\n",
        "    total = left.freq + right.freq\n",
        "    heappush(pq, Huffman(None, total, left, right))\n",
        "\n",
        "  root = pq[0]\n",
        "  code = {}\n",
        "  encode(root, '', code)\n",
        "  if print_tree:\n",
        "    print2DTree(root)\n",
        "  return code, root\n",
        "\n",
        "def decodeString(string, hufTree):\n",
        "  decoded_string = ''\n",
        "  if isLeaf(hufTree):\n",
        "    decoded_string = hufTree.ch * len(string)\n",
        "  else:\n",
        "    i = -1\n",
        "    while i < len(string) - 1:\n",
        "      i, ch = decode(hufTree, i, string)\n",
        "      decoded_string += ch\n",
        "  return decoded_string\n",
        "\n"
      ],
      "metadata": {
        "id": "ObEZzMydxu7F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tests sur la base d'apprentissage du dictionnaire**"
      ],
      "metadata": {
        "id": "XsyVEn-ZfJWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First dictionnary not based on a book\n",
        "\n",
        "compressed = compress(text, tree, dic)\n",
        "\n",
        "# decompressed = decompress(compressed, reverted_dic)\n",
        "\n",
        "code, hufTree = buildHuffmanTree(compressed)\n",
        "\n",
        "compressed_2 = encodeString(compressed, code)\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed)} bits puis {len(compressed_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed) / (len(text) * 8)} puis de {len(compressed_2) / (len(text) * 8)} après Huffman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y394yPYxZrGi",
        "outputId": "8d70fe5e-9326-4887-8fbf-a11310862de7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 5360544 bits puis 4023128 après Huffman\n",
            "Le taux de compression est donc de 0.9848582388994224 puis de 0.7391434072636948 après Huffman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second dictionnary (made with only strings of 3 char)\n",
        "\n",
        "compressed2 = compress(text, tree2, dic2)\n",
        "\n",
        "# decompressed2 = decompress(compressed2, reverted_dic2)\n",
        "\n",
        "code2, hufTree2 = buildHuffmanTree(compressed2)\n",
        "\n",
        "compressed2_2 = encodeString(compressed2, code2)\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed2)} bits puis {len(compressed2_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed2) / (len(text) * 8)} puis de {len(compressed2_2) / (len(text) * 8)} après Huffman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntOWF28ZyVg",
        "outputId": "d9ec642a-26c6-41fa-f118-eceb6f702851"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3422448 bits puis 3056526 après Huffman\n",
            "Le taux de compression est donc de 0.6287843379337713 puis de 0.56155584461396 après Huffman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Third dictionnary (made with strings of 2-3 char)\n",
        "\n",
        "compressed3 = compress(text, tree3, dic3)\n",
        "\n",
        "# decompressed3 = decompress(compressed3, reverted_dic3)\n",
        "\n",
        "code3, hufTree3 = buildHuffmanTree(compressed3)\n",
        "\n",
        "compressed3_2 = encodeString(compressed3, code3)\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed3)} bits puis {len(compressed3_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed3) / (len(text) * 8)} puis de {len(compressed3_2) / (len(text) * 8)} après Huffman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_g1-f2Yk27Z",
        "outputId": "f303eae5-9159-4594-f833-3996fae1db31"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3052683 bits puis 2835925 après Huffman\n",
            "Le taux de compression est donc de 0.5608497949645046 puis de 0.5210262430736217 après Huffman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fourth dictionnary (made with only strings of 2-9 char)\n",
        "\n",
        "compressed4 = compress(text, tree4, dic4)\n",
        "\n",
        "# decompressed4 = decompress(compressed4, reverted_dic4)\n",
        "\n",
        "code4, hufTree4 = buildHuffmanTree(compressed4)\n",
        "\n",
        "compressed4_2 = encodeString(compressed4, code4)\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed4)} bits puis {len(compressed4_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed4) / (len(text) * 8)} puis de {len(compressed4_2) / (len(text) * 8)} après Huffman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc9d4bf-7ae4-4b7b-e985-4ba786566af6",
        "id": "q-cJTpd9Rdn5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3031308 bits puis 2806662 après Huffman\n",
            "Le taux de compression est donc de 0.5569227038229199 puis de 0.5156499404735658 après Huffman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fifth dictionnary (made with only strings of 2-49 char)\n",
        "\n",
        "compressed5 = compress(text, tree5, dic5)\n",
        "\n",
        "# decompressed5 = decompress(compressed5, reverted_dic5)\n",
        "\n",
        "code5, hufTree5 = buildHuffmanTree(compressed5)\n",
        "\n",
        "compressed5_2 = encodeString(compressed5, code5)\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text) * 8} bits et celui compressé fait {len(compressed5)} bits puis {len(compressed5_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed5) / (len(text) * 8)} puis de {len(compressed5_2) / (len(text) * 8)} après Huffman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6331d5-b9ae-4605-9350-a05988573c82",
        "id": "M4mWLXmVReFY"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 5442960 bits et celui compressé fait 3031308 bits puis 2806662 après Huffman\n",
            "Le taux de compression est donc de 0.5569227038229199 puis de 0.5156499404735658 après Huffman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test du dictionnaire sur un autre livre, ici le premier tome du livre \"Les misérables\"**"
      ],
      "metadata": {
        "id": "njuNcCPPTnWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text2 = textract.process(\"dataFrenchWords/hugo_les_miserables_fantine_source.doc\")\n",
        "text2 = text2.decode(\"utf-8\")\n",
        "text2 = text2.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "d3sqDrKnUFrb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed6 = compress(text2, tree5, dic5)\n",
        "\n",
        "# decompressed6 = decompress(compressed6, reverted_dic5)\n",
        "\n",
        "code6, hufTree6 = buildHuffmanTree(compressed6)\n",
        "\n",
        "compressed6_2 = encodeString(compressed6, code6)\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text2) * 8} bits et celui compressé fait {len(compressed6)} bits puis {len(compressed6_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed6) / (len(text2) * 8)} puis de {len(compressed6_2) / (len(text2) * 8)} après Huffman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItGmxIkvUMIn",
        "outputId": "82d7e58c-dc25-4599-bcb6-efa392ffed0b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 6131592 bits et celui compressé fait 3502737 bits puis 3252788 après Huffman\n",
            "Le taux de compression est donc de 0.5712606122520872 puis de 0.5304964844366683 après Huffman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test avec un Roman de Science-fiction**"
      ],
      "metadata": {
        "id": "ShUwl7dLtM7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text3 = textract.process(\"dataFrenchWords/abbot_flatland_source.doc\")\n",
        "text3 = text3.decode(\"utf-8\")\n",
        "text3 = text3.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "eZEGPzmMtMSy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed7 = compress(text3, tree5, dic5)\n",
        "\n",
        "# decompressed7 = decompress(compressed7, reverted_dic5)\n",
        "\n",
        "code7, hufTree7 = buildHuffmanTree(compressed7)\n",
        "\n",
        "compressed7_2 = encodeString(compressed7, code7)\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text3) * 8} bits et celui compressé fait {len(compressed7)} bits puis {len(compressed7_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed7) / (len(text3) * 8)} puis de {len(compressed7_2) / (len(text3) * 8)} après Huffman')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGMug9PDuMFf",
        "outputId": "5ca0090b-b4e5-44b4-b21e-582f49ca69c2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 1955288 bits et celui compressé fait 1122732 bits puis 1035390 après Huffman\n",
            "Le taux de compression est donc de 0.5742028795757965 puis de 0.5295332452303702 après Huffman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test avec des Nouvelles - Contes Humour**"
      ],
      "metadata": {
        "id": "n0GxrW6t01kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text4 = textract.process(\"dataFrenchWords/allais_a_se_tordre_source.doc\")\n",
        "text4 = text4.decode(\"utf-8\")\n",
        "text4 = text4.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "QOCZ06PT01kU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed8 = compress(text4, tree5, dic5)\n",
        "\n",
        "# decompressed8 = decompress(compressed8, reverted_dic5)\n",
        "\n",
        "code8, hufTree8 = buildHuffmanTree(compressed8)\n",
        "\n",
        "compressed8_2 = encodeString(compressed8, code8)\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text4) * 8} bits et celui compressé fait {len(compressed8)} bits puis {len(compressed8_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed8) / (len(text4) * 8)} puis de {len(compressed8_2) / (len(text4) * 8)} après Huffman')"
      ],
      "metadata": {
        "outputId": "aeb4a46b-d5ca-415e-e83b-5facdc7adcda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU-0UfhJ01kU"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 1935936 bits et celui compressé fait 1101645 bits puis 1015447 après Huffman\n",
            "Le taux de compression est donc de 0.5690503198452841 puis de 0.5245250876062019 après Huffman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test avec des Nouvelles - Contes Jeunesse**"
      ],
      "metadata": {
        "id": "ZdX1w3eZ2jPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "text5 = textract.process(\"dataFrenchWords/andersen_contes_tome1_source.doc\")\n",
        "text5 = text5.decode(\"utf-8\")\n",
        "text5 = text5.replace('’', '\\'').replace('–', '-').replace('…', '...').replace('‘', '\\'').replace('€', 'euros').replace('[pic]', '').replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"—\", \"-\")"
      ],
      "metadata": {
        "id": "4Vt0PtfM2jPp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed9 = compress(text5, tree5, dic5)\n",
        "\n",
        "decompressed9 = decompress(compressed9, reverted_dic5)\n",
        "\n",
        "code9, hufTree9 = buildHuffmanTree(compressed9)\n",
        "\n",
        "compressed9_2 = encodeString(compressed9, code9)\n",
        "\n",
        "\n",
        "print(f'Le fichier avant décompression faisait {len(text5) * 8} bits et celui compressé fait {len(compressed9)} bits puis {len(compressed9_2)} après Huffman')\n",
        "print(f'Le taux de compression est donc de {len(compressed9) / (len(text5) * 8)} puis de {len(compressed9_2) / (len(text5) * 8)} après Huffman')"
      ],
      "metadata": {
        "outputId": "f13619f6-d6a1-4738-f029-b25a1f1ab6bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55ZLIF7u2jPp"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le fichier avant décompression faisait 3378528 bits et celui compressé fait 1883835 bits puis 1746725 après Huffman\n",
            "Le taux de compression est donc de 0.5575904654334669 puis de 0.51700770276286 après Huffman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, platform, subprocess, re\n",
        "\n",
        "def get_processor_name():\n",
        "    if platform.system() == \"Windows\":\n",
        "        return platform.processor()\n",
        "    elif platform.system() == \"Darwin\":\n",
        "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin'\n",
        "        command =\"sysctl -n machdep.cpu.brand_string\"\n",
        "        return subprocess.check_output(command).strip()\n",
        "    elif platform.system() == \"Linux\":\n",
        "        command = \"cat /proc/cpuinfo\"\n",
        "        all_info = subprocess.check_output(command, shell=True).decode().strip()\n",
        "        for line in all_info.split(\"\\n\"):\n",
        "            if \"model name\" in line:\n",
        "                return re.sub( \".*model name.*:\", \"\", line,1)\n",
        "    return \"\"\n",
        "\n",
        "get_processor_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2JJFL4cQdf9M",
        "outputId": "6305079f-16b4-4d4a-e292-e12436f8ad4d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Intel(R) Xeon(R) CPU @ 2.30GHz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GPUtil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV9xNc_SeSms",
        "outputId": "d8ab2871-9a3f-4dc9-947d-bc087e953a5f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=1d2cabd7f1171c5fe5c0a57a141eb80f801922341266de1b798fa4dee634f58c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import GPUtil\n",
        "from tabulate import tabulate\n",
        "print(\"=\"*40, \"GPU Details\", \"=\"*40)\n",
        "gpus = GPUtil.getGPUs()\n",
        "list_gpus = []\n",
        "for gpu in gpus:\n",
        "    # get the GPU id\n",
        "    gpu_id = gpu.id\n",
        "    # name of GPU\n",
        "    gpu_name = gpu.name\n",
        "    # get % percentage of GPU usage of that GPU\n",
        "    gpu_load = f\"{gpu.load*100}%\"\n",
        "    # get free memory in MB format\n",
        "    gpu_free_memory = f\"{gpu.memoryFree}MB\"\n",
        "    # get used memory\n",
        "    gpu_used_memory = f\"{gpu.memoryUsed}MB\"\n",
        "    # get total memory\n",
        "    gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
        "    # get GPU temperature in Celsius\n",
        "    gpu_temperature = f\"{gpu.temperature} °C\"\n",
        "    gpu_uuid = gpu.uuid\n",
        "    list_gpus.append((\n",
        "        gpu_id, gpu_name, gpu_load, gpu_free_memory, gpu_used_memory,\n",
        "        gpu_total_memory, gpu_temperature, gpu_uuid\n",
        "    ))\n",
        "print(tabulate(list_gpus, headers=(\"id\", \"name\", \"load\", \"free memory\", \"used memory\", \"total memory\", \"temperature\", \"uuid\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5uFOdGwd9wB",
        "outputId": "8231227b-0af5-49c4-96a0-50cff15a3733"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================== GPU Details ========================================\n",
            "  id  name      load    free memory    used memory    total memory    temperature    uuid\n",
            "----  --------  ------  -------------  -------------  --------------  -------------  ----------------------------------------\n",
            "   0  Tesla T4  0.0%    14845.0MB      264.0MB        15109.0MB       67.0 °C        GPU-86000074-0d56-4dc4-0d14-c9723bd485a8\n"
          ]
        }
      ]
    }
  ]
}